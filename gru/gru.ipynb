{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from IPython.display import clear_output\n",
    "\n",
    "path_to_data = '../Inputs-Targets.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, csv_file, train=True):\n",
    "        # Make 80/20 test training split without preserving order\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.train = train\n",
    "        if train:\n",
    "            self.data = self.data.iloc[:int(len(self.data)*0.8)]\n",
    "        else:   \n",
    "            self.data = self.data.iloc[int(len(self.data)*0.8):]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Return row as tensor\n",
    "        x = torch.tensor(self.data.iloc[i][1:69])\n",
    "        y = torch.tensor(self.data.iloc[i][69:72]).to(torch.float64)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trivial model\n",
    "class WeatherGRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeatherGRU, self).__init__()\n",
    "        self.fc1 = nn.Linear(68, 256, dtype=torch.float64)\n",
    "        self.fc2 = nn.Linear(256, 256, dtype=torch.float64)\n",
    "        self.fc3 = nn.Linear(256, 256, dtype=torch.float64)\n",
    "        self.fc4 = nn.Linear(512, 256, dtype=torch.float64)\n",
    "        self.fcf = nn.Linear(256, 3, dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x_prime = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(torch.cat([self.fc3(x_prime), x_prime], dim=1))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return self.fcf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_session(model, path='model.pt', stat_dict={}):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    torch.save(stat_dict, 'stat_dict.pt')\n",
    "\n",
    "\n",
    "def restore_session(model, model_path='model.pt', stat_dict_path='stat_dict.pt'):\n",
    "   # Restore model and stats from cache if available, else initialize new model and stats\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        stat_dict = torch.load(stat_dict_path)\n",
    "        losses = stat_dict['losses']\n",
    "        window_losses = stat_dict['window_losses']\n",
    "        val_losses = stat_dict['val_losses']\n",
    "        epoch_num = stat_dict['epoch_num']\n",
    "        print('Restored session from cache at epoch {}'.format(epoch_num))\n",
    "    except:\n",
    "        losses = []\n",
    "        window_losses = []\n",
    "        val_losses = []\n",
    "        epoch_num = 0\n",
    "        print('Unable to restore. Initialized new model and new session')\n",
    "    return model, epoch_num, losses, window_losses, val_losses\n",
    "\n",
    "def plot_loss(losses, loss_indices):\n",
    "  for i, l in enumerate(losses):\n",
    "    loss_label = l[0]\n",
    "    loss_x = loss_indices[i]\n",
    "    loss_y = l[1]\n",
    "    plt.plot(loss_x, loss_y, label=loss_label)\n",
    "  plt.legend()\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Batches')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Make train and test datasets\n",
    "train_data = WeatherDataset(path_to_data, train=True)\n",
    "test_data = WeatherDataset(path_to_data, train=False)\n",
    "# Initialize dataloaders\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_STATUS_EVERY = 200\n",
    "\n",
    "# Train model\n",
    "def train(epochs, model, epoch_num=0, losses_tuple=([], []), window_losses_tuple=([], []), val_losses_tuple=([], [])):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    total_batch_index = 0\n",
    "    \n",
    "    loss_indices = losses_tuple[0]\n",
    "    losses = losses_tuple[1]\n",
    "    window_loss_indices = window_losses_tuple[0]\n",
    "    window_losses = window_losses_tuple[1]\n",
    "    val_loss_indices = val_losses_tuple[0]\n",
    "    val_losses = val_losses_tuple[1]\n",
    "\n",
    "    # epochs_bar = tqdm(range(epochs), leave=False, position=epoch_num)\n",
    "\n",
    "    for epoch in range(epoch_num, epochs):\n",
    "        \n",
    "        # Reset output for current epoch\n",
    "        clear_output(wait=True)\n",
    "        epochs_bar = tqdm(range(epochs), leave=False, initial=epoch) # Reset epochs bar\n",
    "        try:\n",
    "            epochs_bar.set_description(f'Epoch {epoch + 1}, Train Loss: {f\"{recent_avg_loss:.3f}\"}, Val Loss: {f\"{avg_val_batch_loss:.3f}\"}')\n",
    "        except:\n",
    "            epochs_bar.set_description(f'Epoch {epoch_num + 1}, Train Loss: N/A, Val Loss: N/A')\n",
    "\n",
    "        batches_bar = tqdm(train_dataloader, leave=False)\n",
    "        plot_loss(losses=(('Window Loss', window_losses), ('Val Loss', val_losses)),\n",
    "                   loss_indices=(window_loss_indices, val_loss_indices)) # Plot current losses\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_indices.append(total_batch_index)\n",
    "            losses.append(loss.item())\n",
    "            batches_bar.update()\n",
    "            total_batch_index += 1\n",
    "\n",
    "            # Update batch bar periodically during batch\n",
    "            if batches_bar.n % UPDATE_STATUS_EVERY == 0:\n",
    "                recent_avg_loss = sum(losses[-UPDATE_STATUS_EVERY:])/UPDATE_STATUS_EVERY\n",
    "                window_loss_indices.append(total_batch_index)\n",
    "                window_losses.append(recent_avg_loss)\n",
    "                batches_bar.set_description(f'Epoch {epoch + 1}, Train Loss: {f\"{recent_avg_loss:.3f}\"}')\n",
    "\n",
    "        # Validate every epoch\n",
    "        val_batches_bar = tqdm(test_dataloader, leave=False)\n",
    "        val_batches_bar.set_description(f'Validating...')\n",
    "        val_batches_bar.update()\n",
    "        val_batch_losses = []\n",
    "        model.eval()\n",
    "        for batch in test_dataloader:\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            val_batch_losses.append(loss.item())\n",
    "            val_batches_bar.update()\n",
    "        avg_val_batch_loss = sum(val_batch_losses)/len(val_batch_losses)\n",
    "        val_loss_indices.append(total_batch_index)\n",
    "        val_losses.append(avg_val_batch_loss)\n",
    "        model.train()\n",
    "\n",
    "        # Save model and losses\n",
    "        stat_dict = {'epoch_num': epoch + 1,'losses': (loss_indices, losses), 'window_losses': (window_loss_indices, window_losses), 'val_losses': (val_loss_indices, val_losses)}\n",
    "        cache_session(model ,stat_dict=stat_dict)\n",
    "\n",
    "        # Update epoch bar\n",
    "        epochs_bar.update()\n",
    "\n",
    "model = WeatherModel()\n",
    "# model = Res50Model()\n",
    "EPOCHS = 100\n",
    "do_restore = False\n",
    "if do_restore:\n",
    "    train(EPOCHS, *restore_session(model))\n",
    "else:\n",
    "    print('Initialized new model and new session')\n",
    "    train(EPOCHS, model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
